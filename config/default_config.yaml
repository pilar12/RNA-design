resume_training: False

experiment:
  experiments_base_dir: /home/ubuntu/workspace/experiments
  project_name: RNAformer
  session_name: rnaformer_1
  experiment_name: ts0_conform_test-0002

trainer:
  num_nodes: 1
  check_val_every_n_epoch: null #1
  default_root_dir: /home/ubuntu/workspace/
  enable_checkpointing: true
  enable_model_summary: true
  enable_progress_bar: true
  accelerator: 'gpu'
  devices: 1
  gradient_clip_val: 0.1
  limit_train_batches: null
  limit_val_batches: null
  log_every_n_steps: 10
  max_epochs: null
  max_steps: 30000
  num_sanity_val_steps: 2
  precision: 16
  reload_dataloaders_every_n_epochs: 1
  replace_sampler_ddp: false
  resume_from_checkpoint: null
  track_grad_norm: -1
  val_check_interval: 10
  
train:
  seed: 1234
  neg_samples: False # 500  # calculates the softmax CE loss only over #neg_samples+1 words
  softmax_temp: False  # calculates the softmax with temperature

  optimizer:
    _target_: torch.optim.AdamW
    lr: 0.001
    weight_decay: 0.1
    betas:
      - 0.9
      - 0.98
    eps: 1.0e-09
    adam_w_mode: true
    seed: 1234
    scheduler_mult_factor: null
  optimizer_param_grouping:
    bias_weight_decay: False
    normalization_weight_decay: False
  scheduler:
    num_training_steps: ${trainer.max_steps}
    num_warmup_steps: 1000 #${eval:0.01 * ${trainer.max_steps}}
    decay_factor: 0.1
    schedule: "cosine" # "cosine" or "linear"
  loss_fn:
    inplace_backward: True  # to save memory
  val_fold_test: True  
  n_samples: 5
  val_fold_test_freq: 2
  temperature: 1.0
  max_len: ${rna_data.max_len}
  devices: ${trainer.devices}
  batch_size: ${rna_data.batch_size}

callbacks:
  learning_rate_monitor:
    _target_: pytorch_lightning.callbacks.LearningRateMonitor
    logging_interval: step
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    dirpath: ${experiment.experiments_base_dir}
    auto_insert_metric_name: False
    every_n_train_steps: 10000
    every_n_epochs: null
    save_top_k: 1
    monitor: "step"
    mode: "max"
    filename: "checkpoint-{epoch:02d}-{global_step}"
    save_last: True


logger:
  tensorboard:
    _target_: pytorch_lightning.loggers.tensorboard.TensorBoardLogger
    save_dir: "tensorboard/"
    name: ""
    version: "tb"
    log_graph: False
    default_hp_metric: True
    prefix: ""

RNADesignFormer:
  precision: ${trainer.precision}
  seq_vocab_size: 0
  struct_vocab_size: 0
  src_vocab_size: 0
  trg_vocab_size: 0
  max_len: ${rna_data.max_len}
  
  gc_conditioning: False
  energy_conditioning: False

  model_dim: 256 # hidden dimension of transformer
  n_layers: 6  # number of transformer layers
  num_head: 4  # number of heads per layer
  
  ff_factor: 4  # hidden dim * ff_factor = size of feed-forward layer
  ff_kernel: 3

  resi_dropout: 0.1
  embed_dropout: 0.1
  attn_dropout: 0.1

  flash_attn: False

  rel_pos_enc: False  # relative position encoding
  head_bias: False
  ln_eps: 1e-5
  sym_pos_enc: False

  softmax_scale: True
  key_dim_scaler: True
  gating: False
  use_glu: False
  use_bias: true
  matrix: False
  matrix_inp: False
  mtype: cat

  flash: False
  
  initializer_range: 0.02
  zero_init: false  # init last layer per block before each residual connection

test:
  cache_dir: ${rna_data.cache_dir}
  datasets: [synthetic_test]
  batch_size: ${rna_data.batch_size}
  n_samples: 20

rna_data:
  dataframe_path: "data/rfam/synthetic_data_all_conform_final.plk.zip"
  valid_sets: [synthetic_valid]
  test_sets: [synthetic_test]
  oversample_pdb: 1
  predict_canonical: false
  random_ignore_mat: 0
  partial_training: false
  design: True
  num_cpu_worker: 10
  num_gpu_worker: ${trainer.devices}
  min_len: 2
  max_len: 100
  min_gc: 0
  max_gc: 1.0
  min_energy: -1000
  max_energy: 1.0
  similarity: 80
  seed: 1
  batch_size: 128
  finetune_pdb: False
  finetune_pk: False
  batch_by_token_size: False
  batch_token_size: 10000
  shuffle_pool_size: 20
  cache_dir: "data/rfam"
  matrix_collate: ${RNADesignFormer.matrix}

riboswitch_data:
  cache_dir: "data/riboswitch"
  test:
    datasets: [ribo_test]
    n_samples: 20

pd_train_data:
  train_dataframe_path: "data/riboswitch/rfam_pd_short_train.plk"
  valid_dataframe_path: "data/riboswitch/rfam_pd_validation.plk"
  valid_sets: ['valid']
  test_sets: []
  design: True
  num_cpu_worker: 10
  num_gpu_worker: ${trainer.devices}
  min_len: ${rna_data.min_len}
  max_len: ${rna_data.max_len}
  seed: ${rna_data.seed}
  batch_size: ${rna_data.batch_size}
  batch_by_token_size: ${rna_data.batch_by_token_size}
  batch_token_size: ${rna_data.batch_token_size}
  shuffle_pool_size: ${rna_data.shuffle_pool_size}
  matrix_collate: ${RNADesignFormer.matrix}
  cache_dir: "data/riboswitch"

syn_ns_data:
  dataframe_path: "data/syn_ns/syn_rnafold.plk"
  valid_sets: [valid]
  test_sets: [test]
  cache_dir: "data/syn_ns"
  test:
    datasets: [syn_ns_test]
    n_samples: 20

syn_hk_data:
  dataframe_path: "data/syn_hk/syn_hk.plk"
  valid_sets: [valid]
  test_sets: [test]
  cache_dir: "data/syn_hk"
  test:
    datasets: [syn_hk_test]
    n_samples: 20

syn_multi_data:
  dataframe_path: "data/syn_pdb/syn_pdb.plk"
  valid_sets: [valid]
  test_sets: []
  cache_dir: "data/syn_pdb"
  test:
    datasets: [syn_multi_test, pdb_ts1_test, pdb_ts2_test, pdb_ts3_test, pdb_ts_hard_test]
    n_samples: 100
